# app.py
# ç®€åŒ–ç‰ˆ Streamlit ä¸­æ–‡å¸ƒå°”æ£€ç´¢ Dashboard
# - åªæŒ‰â€œè¯â€æ£€ç´¢ï¼ˆå¿½ç•¥è¯æ€§ï¼‰
# - åªå±•ç¤ºå¸¦è¯æ€§çš„åŸå¥
# - æ”¯æŒ AND / OR / NOTï¼ˆä»å·¦åˆ°å³é¡ºåºæ‰§è¡Œï¼ŒNOT ä¸ºä¸€å…ƒï¼‰
# - æä¾›â€œæ£€ç´¢â€æŒ‰é’® + Enter æäº¤
# è¿è¡Œï¼šstreamlit run app.py

import re
import requests
import pandas as pd
import streamlit as st
import altair as alt
from collections import defaultdict

st.set_page_config(page_title="æ£€ç´¢ Dashboard", layout="wide")

# st.title("ğŸ” ä¸­æ–‡æ–‡æœ¬æ£€ç´¢ Dashboard")
st.caption("ä» GitHub è¯»å–å¸¦è¯æ€§æ ‡æ³¨çš„æ–‡æœ¬ï¼Œé€šè¿‡å¸ƒå°”é€»è¾‘ (AND/OR/NOT) æ£€ç´¢å¥å­ã€‚")

# ------------------------
# åœ¨è¿™é‡Œå†…ç½®ä½ çš„ GitHub RAW æ–‡æœ¬åœ°å€
# ------------------------
GITHUB_FILES = {
    "è·¯é¥ã€Šå¹³å‡¡çš„ä¸–ç•Œã€‹": "https://raw.githubusercontent.com/Dittonal/streamlit_search_txt/main/è·¯é¥ã€Šå¹³å‡¡çš„ä¸–ç•Œã€‹_pos.txt",
    "è€èˆã€Šéª†é©¼ç¥¥å­ã€‹": "https://raw.githubusercontent.com/Dittonal/streamlit_search_txt/main/è€èˆã€Šéª†é©¼ç¥¥å­ã€‹_pos.txt",
    "ç‹å®‰å¿†ã€Šé•¿æ¨æ­Œã€‹": "https://raw.githubusercontent.com/Dittonal/streamlit_search_txt/main/ç‹å®‰å¿†ã€Šé•¿æ¨æ­Œã€‹_pos.txt",
    "å¼ çˆ±ç²ã€ŠåŠç”Ÿç¼˜ã€‹": "https://raw.githubusercontent.com/Dittonal/streamlit_search_txt/main/å¼ çˆ±ç²ã€ŠåŠç”Ÿç¼˜ã€‹_pos.txt",
}
@st.cache_data(show_spinner=False)
def fetch_text(url: str) -> str:
    try:
        r = requests.get(url, timeout=10)
        if r.status_code == 200:
            r.encoding = r.apparent_encoding or "utf-8"
            return r.text
    except Exception:
        pass
    return ""

def split_sentences(text: str):
    """ç®€å•ä¸­æ–‡åˆ†å¥"""
    text = re.sub(r"[ \t]+", " ", text.strip())
    return [s.strip() for s in re.split(r"[ã€‚ï¼ï¼Ÿ!?ï¼›;]\s*|\n+", text) if s.strip()]

def get_words(sentence: str):
    """æå–è¯ï¼ˆå¿½ç•¥è¯æ€§ï¼‰"""
    words = []
    for t in sentence.split():
        if "/" in t:
            w, _ = t.split("/", 1)
            words.append(w)
        else:
            words.append(t)
    return words

def eval_query(query: str, words: list):
    """
    æœ€ç®€ç‰ˆå¸ƒå°”é€»è¾‘ AND/OR/NOTï¼š
    - NOT ä¸ºä¸€å…ƒï¼Œä½œç”¨äºå…¶åç´§éšçš„ä¸€ä¸ªå¸ƒå°”å€¼ï¼ˆè¿™é‡Œç›´æ¥ç¿»è½¬ä¸Šä¸€ä¸ªå…¥æ ˆå¸ƒå°”ï¼‰
    - AND / OR æŒ‰ä»å·¦åˆ°å³é¡ºåºæ‰§è¡Œï¼ˆæ— æ‹¬å·ä¼˜å…ˆçº§ï¼‰
    - ä»…æŒ‰â€œè¯â€åŒ¹é…ï¼ˆå¿½ç•¥è¯æ€§ï¼‰
    """
    q = query.upper().replace("(", " ( ").replace(")", " ) ")
    tokens = [t for t in q.split() if t]

    lw = [w.lower() for w in words]

    def term_match(term):
        return term.lower() in lw

    stack = []
    for t in tokens:
        if t == "NOT":
            if stack and isinstance(stack[-1], bool):
                stack[-1] = not stack[-1]
            else:
                # ç®€å•å¤„ç†ï¼šè‹¥ NOT å‰æ— å¸ƒå°”å€¼ï¼Œåˆ™å°†ä¸€ä¸ªå ä½ False ç¿»è½¬ä¸º True
                stack.append(True)
        elif t == "AND":
            stack.append("AND")
        elif t == "OR":
            stack.append("OR")
        elif t in ("(", ")"):
            # ç®€åŒ–ï¼šå¿½ç•¥æ‹¬å·
            continue
        else:
            stack.append(term_match(t))

    # é¡ºåºè®¡ç®— AND / OR
    result = None
    op = None
    for item in stack:
        if isinstance(item, bool):
            if result is None:
                result = item
            elif op == "AND":
                result = result and item
            elif op == "OR":
                result = result or item
        else:
            op = item
    return bool(result)

# ------------------------
# è¾“å…¥è¡¨å•ï¼šç‚¹å‡»æŒ‰é’®æˆ–æŒ‰ Enter å¼€å§‹æ£€ç´¢
# ------------------------
with st.form("search_form", clear_on_submit=False):
    query = st.text_input("è¾“å…¥æ£€ç´¢è¯ï¼ˆæ”¯æŒ AND / OR / NOTï¼‰ï¼š", value="å¥³äºº AND çˆ±")
    submitted = st.form_submit_button("ğŸ” æ£€ç´¢")

# åªæœ‰åœ¨ç‚¹å‡»æŒ‰é’®æˆ–æŒ‰ Enter æäº¤åæ‰æ‰§è¡Œæ£€ç´¢
if submitted:
    # ä¸‹è½½è¯­æ–™
    corpus = {}
    sentences_map = {}
    for name, url in GITHUB_FILES.items():
        raw = fetch_text(url)
        corpus[name] = raw
        sentences_map[name] = split_sentences(raw) if raw else []

    if not any(sentences_map.values()):
        st.warning("æœªèƒ½ä»å†…ç½®çš„ GitHub RAW é“¾æ¥æ‹‰å–åˆ°æ–‡æœ¬ã€‚è¯·åœ¨ä»£ç ä¸­æ›¿æ¢ä¸ºæœ‰æ•ˆçš„ RAW åœ°å€åé‡è¯•ã€‚")
        st.stop()

    # æ£€ç´¢ä¸ç»Ÿè®¡
    rows = []
    match_counts = defaultdict(int)

    for fname, sents in sentences_map.items():
        for idx, s in enumerate(sents, start=1):
            words = get_words(s)  # åªæŒ‰è¯åŒ¹é…
            if eval_query(query, words):
                rows.append({"æ–‡ä»¶": fname, "å¥å·": idx, "å¥å­ï¼ˆå«è¯æ€§ï¼‰": s})
                match_counts[fname] += 1

    # Dashboard æŒ‡æ ‡
    total = sum(match_counts.values())
    cols = st.columns(5)
    cols[0].metric("æ€»åŒ¹é…å¥å­æ•°", total)
    for i, name in enumerate(GITHUB_FILES.keys()):
        cols[i+1].metric(name, match_counts.get(name, 0))

    # å›¾è¡¨ï¼ˆæ¨ªåæ ‡æ–‡å­—ä¸æ—‹è½¬ï¼‰
    summary = pd.DataFrame({
        "æ–‡ä»¶": list(GITHUB_FILES.keys()),
        "åŒ¹é…å¥å­æ•°": [match_counts.get(n, 0) for n in GITHUB_FILES.keys()]
    })
    # ç»“æœè¡¨æ ¼
    st.markdown("### æ£€ç´¢ç»“æœï¼ˆå«è¯æ€§ï¼‰")
    if rows:
        st.dataframe(pd.DataFrame(rows), use_container_width=True, hide_index=True)
    else:
        st.info("æœªæ£€ç´¢åˆ°åŒ¹é…ç»“æœã€‚")
else:
    st.info("è¾“å…¥æ£€ç´¢è¯åï¼Œç‚¹å‡» **ğŸ” æ£€ç´¢** æˆ–åœ¨è¾“å…¥æ¡†æŒ‰ **Enter** å¼€å§‹ã€‚ä¾‹ï¼š`å¥³äºº AND çˆ±`ã€`å¥³äºº OR çˆ±`ã€`å¥³äºº AND NOT çˆ±`ã€‚")
